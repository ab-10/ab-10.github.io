---
layout: post
title: "The unreasonable effectivness of language models"
draft: true
categories: Reflections
---


Based on the name alone, it would seem that _language model_ refers to any machine learning model that operates on language.
That is not entirely true.
Instead language models are a particular type of machine learning models trained on a very simple task, predicting missing words in text.
Suprisingly, this set of models has become the de facto model for _all_ language tasks.
This is the _unreasonable_ effectivness of language models.
<!-- Maybe  frame this article from the perspective _the simple task at the core of ChatGPT_-->
In this post I will explore why a simple task has become the core of nearly all contemporary [NLP](https://arminbagrat.com/What-on-Earth-is-Natural-Language-Processing/) applications and ponder what are it's limits.

# What are language models?

As mentioned before, a language model is a machine learning model trained to predict missing words in text.
But what exactly does that mean?

// Obvious advantages: abundance of textual data.

# Why do language models work?

# What are the linguistic motivations for language models?

- What is the linguistic basis of the distributional hypothesis?
Guy Emerson seems to have interesting work on distributional semantics: https://www.languagesciences.cam.ac.uk/directory/guy-emerson

A nuanced contemporary discussion of distributional semantics: https://aclanthology.org/2022.naacl-main.327.pdf

# Why can neural networks learn how to generate coherent sequences of words, despite funndamental mathematical limitations?

- If we were to 

# What can language models do?

# What are the limitations of language models?
